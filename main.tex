% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,amsfonts,clevref,
 aps,
%pra,
%prb,
%rmp,
prstab,
%prstper,
%floatfix,
]{revtex4-2}
\usepackage{wasysym}
\usepackage{graphicx, subcaption}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage[allcolors=blue]{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

% \usepackage[active, tightpage]{preview}
% \setlength\PreviewBorder{0pt}
% \PreviewSnarfEnvironment[{[]}]{figure}

%\usepackage[active,tightpage,floats]{preview}

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines
% \usepackage{epic,eepic,pspicture,mathptmx,times,graphpap,siunitx, pgf, import}
% For algorithms
%\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{siunitx, pgf, import}
\usepackage[super]{nth}
\newcommand{\shirinkimage}[2]{\resizebox{#1\textwidth}{!}{\input{#2}}}
\usepackage{cleveref}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning} 
\tikzset{
    %Define standard arrow tip
    >=stealth',
    %Define style for boxes
    punkt/.style={
           rectangle,
           rounded corners,
           draw=black, very thick,
           text width=3.5em,
           minimum height=2em,
           text centered},
    % Define arrow style
    pil/.style={
           ->,
           thick,
           shorten <=2pt,
           shorten >=2pt,}
}
%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\usepackage{wasysym}
%\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}

\newcommand{\NB}[1]{\textcolor{red}{#1}}
\include{tex/defs}

\begin{document}

\preprint{APS/123-QED}



\title{Model based and model free reinforcement learning at FERMI FEL}



\author{Simon Hirlaender}
%\thanks{These two authors contributed equally}
% \affiliation{University of Malta}
\affiliation{Univertsity of Salzburg, Salzburg, Austria}
\author{Niky Bruchon}%, Gianfranco Fenu, Felice Andrea Pellegrino, Erica Salvato}
\affiliation{University of Trieste, Trieste, IT}
% \author{Gianluca Valentino}
% \affiliation{University of Malta, Msida, MT}
% \author{Marco Lonza, Giulio Gaio}
% \affiliation{Elettra Sincrotrone Trieste, Trieste, IT}
% \author{Verena Kain}
% %\thanks{These two authors contributed equally}
% \email{verena.kain@cern.ch}

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified


\begin{abstract}
In this paper we discuss a model based reinforcement learning approach in comparison to a model free reinforcement learning approach applied at the FERMI FEL system. Both algorithms and approaches are new in this context and the main purpose of this paper is to show how reinforcement learning can be applied on an operational level in a feasible training time on real accelerator physics problems. In terms of sample-complexity the model-based approach is faster, while the final performance of the model free method is superior - the so called asymptotic performance. The model-based algorithm is done in a Dyna-style using an uncertainty aware model and the model-free algorithm is based on tailored deep Q-learning using some tricks to increase the sample efficiency.
The implementation is available at \cite{Hirlaender2020}.

\end{abstract}
%\keywords{Suggested keywords}%Use showkeys class option if keyword
                              %display desired
\maketitle
\begin{itemize}
	\item Noise benchmarks with pendulum
	\item Benchmark NAF2 with pendulum
	\item The first time model based on accelerator problem compared to a novel state of the art MFRL: NAF2
	\item Tailored to accelerators - short horizons are standard?
	\item Improvements of the algorithms:
	\begin{itemize}
		\item Adding the noise feature
		\item Jumping feature using SAC
		\item Noise in the policy
		\item discuss different approaches to e.g. dynamic waiting time
	\end{itemize}
\end{itemize}
%\tableofcontents
\section{Introduction and Motivation}
In particle accelerators one main goal is to provide stable and reproducible performance. In order to achieve this, a number of control problems have to be considered simultaneously. Especially if there is no way to model the physics, one might use optimisation techniques as e.g. derivative free optimisers (DFOs) or model-based optimisations as Gaussian processes to restore or maintain the performance. Another promising way is to apply reinforcement learning which shows several advantages over optimisation methods:
\begin{itemize}
    \item It covers a larger class of problems, RL optimises a sequence of decisions.
    \item It memorizes the problem and does not start always from scratch as an DFO.
    \item Existing data can be used.
    \item The underlying structure of the problem might be deduced.
\end{itemize}
On critical aspect using RL is the number of iteration to train a controller.
In this paper, we present the study carried out to solve the maximisation-problem of the radiation intensity generated by a seeded Free-Electron Laser (FEL) on the Free Electron laser Radiation for Multidisciplinary Investigations (FERMI) at Elettra Sincrotrone Trieste. Three algorithms were applied successfully showing different advantages, which will e discussed in the following.

% The methods proposed are the iterative Linear Quadratic Regulator (iLQR) - an extension to non-linear systems of the fundamental control theory Linear Quadratic Regulator (LQR) problem -  and the Normalized Advantage Function (NAF) - a continuous variant of the Q-learning algorithm from reinforcement learning field. An exhaustive presentation of both techniques is provided in the following sections.

In a seeded free-electron laser one of the most critical parameters is the temporal and transverse overlap of the electron and laser beam in the magnetic section called modulator undulator. 
\section{The physical set-up}
At FERMI several beam-based feedback systems are deployed to control the beams trajectories shot to shot with the main purpose of guaranteeing a steady intensity of the light radiation. Nevertheless, in the last years various approaches have been studied to investigate their applicability in tuning operations. 
%%% Last introduction paragraph
The paper is organised as follows. 
\begin{itemize}
    \item Description of the problem set-up at FERMI
    \item Overview of RL
    \item Details of the implementations used in these studies and theoretical concerns
    \item Results 
    \item Summary and outlook
\end{itemize}

\section{Deep Reinforcement learning}
Assume states $\bs$ the set of all states $\states$ and actions $\ba$ all actions $\actions$ and rewards $\reward$ and the set of all rewards $\mathcal{R}$ an initial state distribution $\initstate$.
Goal of reinforcement learning is to find a $\policy: \bs\mapsto \ba$, which is the solution of:
\begin{align}
\max J (\policy)  =  \mathbb E_{\traj \sim p_{\policy}}\left[\sum_{t=0}^{\horizon}\discount^t r(\bs_t,\ba_t)\right],\label{eq:cumulative_reward}
\end{align}
where $p_\policy = \initstate(\bs_0)\prod_{t_0}^\horizon\policy(\ba_t,\bs_t)\transitions(\bs_{t+1}|\bs_t,\ba_t)$ is the distribution of all trajectories $\traj := (\bs_0, \ba_0, \bs_1, \ba_1,\dots \bs_\horizon,\ba_\horizon)$ drawn by $\policy$ with a horizon $\horizon$.
In the modern field of RL one distinguishes if the policy $\policy(\ba)\approx \policy_\phi(\bs)$ or the state-action value function $Q(\bs,\ba)$ is approximated using a high capacity function approximator, as e.g. a deep neural network. In the first case one speaks about policy gradient methods in the latter about approximate dynamic programming, which we now discuss.
\subsection{Approximate dynamic programming}
The state-value function $Q(\bs,\ba)$ is expressed as $Q_\theta(\bs,\ba)$, where $\theta$ denotes the parameters of the function approximator. By satisfying the Bellmann-optimality equation $Q_\theta$ can be trained towards the optimal $Q^*(\bs,\ba)$:
\begin{align}
    \min_\theta \left(\vec Q_\theta - \bellman^*\vec Q_\theta\right)^2.\label{eq:minimize_bellmann_optimality}
\end{align}
And $\policy$ can be calculated via:
\begin{align}
	\policy_\theta(\bs)=(\delta(\ba)-\arg\!\max_\ba Q_\theta(\bs,\ba)).
\end{align}
The Bellman operator $\bellman^*$ has a unique fixed point but is non-linear, due to the $\max$ - operator:
\begin{align}
 \bellman^*\vec Q_\theta(\bs_t,\ba_t) := r(\bs_t,\ba_t)+\gamma\max_\ba\left( Q_\theta(\bs_{t+1},\ba) - Q_\theta(\bs_t,\ba_t)\right).
\end{align}
The form of this equation can cause overestimation and other complications, when using a function approximator. Several methods exist which try to mitigate the problems \cite{}.
One way to reduce the effect is to take a simple analytical form of the $Q$-function.
\subsection{Normalized advantage function}
If a specific quadratic form of the $Q$ function is assumed:
\begin{align}
     Q_\theta(\bs,\ba) = -\frac{1}{2}(\ba-\mu_\theta(\bs))P_\theta(\ba-\mu_\theta(\bs))^T+V_\theta(\bs).\label{eq:state-action-value-approxiation}
\end{align}

One modification, which is used in these tests is a twin network (weights for network $i$ denoted by $\theta^i$), where only one is used to obtain the policy, while the other is used for the update rule to avoid over-estimation. It is motivated by double Q-learning \cite{NIPS2010_091d584f}.
The maximum is given analytically as $\max_\ba Q(\bs,\ba) = V(\bs)$, hence from \cref{eq:minimize_bellmann_optimality}:
\begin{align}
	\min_\theta\left( (\reward(\bs_t,\ba_t)+\gamma \min_{1,2} V_{\theta^i_\text{targ}}(\bs_{t+1}) - (1+\gamma) Q_\theta(\bs_t,\ba_t)\right)^2
\end{align}
$\theta_\text{targ}$ are the weights of a target network, which is softly updated. To stabilize the network training a small artificial noise is added to the actions in the update. This algorithm has an extremely good sample-efficiency for suitable problems as can be found often in accelerators and is used as the baseline for the considered control problem, as it shows very good results.
%\subsection{New NAF network architecture}

\begin{algorithm}[ht]
\caption{On-policy policy gradient with Monte Carlo estimator \label{alg:pg}}
\begin{algorithmic}[1]
\State initialise $\theta_0$
\For{iteration $k \in [0, \dots, K]$}
\State sample trajectories $\{\traj_i\}$ by running $\policy_{\theta_k}(\ba_t|\bs_t)$ \Comment{each $\traj_i$ consists of $\bs_{i,0},\ba_{i,0},\dots,\bs_{i,H},\ba_{i,H}$}
%\State compute $\return_{i,t} = \sum_{t'=t}^H \discount^{t'-t} \reward(\bs_{i,t},\ba_{i,t})$
%\State fit $b(\bs_t)$ to $\{\return{i,t}\}$ \Comment{use constant $b_t = \frac{1}{N}\sum_i \return{i,t}$, or fit $b(\bs_t)$ to $\{\return{i,t}\}$}
%\State compute $\hat{A}(\bs_{i,t},\ba_{i,t}) = \return_{i,t} - b(\bs_t)$
%\State estimate $\nabla_{\theta_k} J(\policy_{\theta_k}) \approx \sum_{i,t} \nabla_{\theta_k} \log \policy_{\theta_k}(\ba_{i,t} | \bs_{i,t}) \hat{A}(\bs_{i,t},\ba_{i,t})$
%\State update parameters: $\theta_{k+1} \leftarrow \theta_k + \alpha \nabla_{\theta_k} J(\policy_{\theta_k})$
%\EndFor
\end{algorithmic}
\end{algorithm}

\section{Uncertainty aware Dyna-style reinforcement learning}
The original Dyna algorithm \cite{Kurutach2018} is modified here in several aspects. Generally, Dyna style algorithms \cite{Sutton1991} denote algorithms, where a MFRL algorithm is trained on purely synthetic data from an approximate dynamics model or on a mixture of synthetic and real data. We use only synthetic data to reduce the interaction with the real environment to a minimum.
An overview of the used method is shown in \cref{fig:MBRL_overview}. At the beginning the data is collected or read in from a pre-collection. An uncertainty aware model is trained, using anchored ensembles \cite{Pearce2018} on the data, which allows to take the allegorical (measurement errors) as well as the epistemic uncertainty (lack of data) into account. Subsequently a MFRL algorithm is deployed to learn the controller on the learned model by only using the synthetic data, by taking the uncertainty into account. After a defined number of training steps the controller is testes on each model individually. If there is no improvement in a specific ratio$<1$ of models, the controller is tested on the real environment for a number of episodes. The training is stopped if the controller solves the problem on the real environment.

\begin{figure}[!h]
	\centering
	\includegraphics*[width=0.5\textwidth]{Figures/MBRL_overview}
	\caption{A schematic overview of the AE-Dyna approach used in this paper.}
	\label{fig:MBRL_overview}
\end{figure}
\subsection{Critical design decisions}\label{ss:critical_design}
In the following the most important aspects of a successful application of a Dyna-AE algorithm are listed:
\begin{itemize}
	\item The ANN - including the prior - number of models - noise level - early stopping
	\item The number of data points and the policy.
	\item The uncertainty - how is this included?
	\item The episodic design - avoid long trajectories.
	\item The MFRL agent as e.g. the TRPO and PPO or the SAC and its training
	\item Tuning the algorithm on a model.
\end{itemize}
We try to discuss all items in some detail in the following.
The anchoring ensemble methods usually yields good results already with a small number of different networks. The idea behind it, is to mimic the posterior probability of the dynamics model to capture first of all its own uncertainty due to the lack of data. Empirical results show that three to five models were sufficient to see clear advantages over a single network approach and the main goal is not to calculate the exact posterior of the model. A small two-layer network with around 15-25 nodes and the $tanh$ activation were used. The last layer was linear and the inputs were normalized to the interval [-1,1]. The prior is controlled by the initialization of the weights, which were normalized by the number of nodes in each layer. The noise level is also respected during the initialization. For the moment only homeoscetastic errors are used. Several methods to set the training method were implemented. Early stopping, a standard techniques showed good results, mainly to the fact, that the number of training steps is increasing with more data. The advantage is that the uncertainty at regions without data shrink in this way leading to a better training performance. A fixed loss value threshold was also tested, as also used in the original anchore-ensemble technique. In the experiments a combination of both was taken.\\
One of the most crucial points is how this uncertainties are taken into account by the RL algorithms. Several different approaches were tried. It is possible to take the average of the models and add Gaussian noise leveled by the standard deviation of the models. Another straight forward way is to randomly select a model to provide at each training step, which is the original implementation of the ME-TRPO algorithm and was used in the experiments labeled \emph{ME-TRPO}. A pessimistic setting only would select the model resulting in the lowest predicted reward. Good results were obtained by following a randomly selected single model each full episode and were used in the experiments labeled \emph{AE-Dyna}.\\
The number of data-points taken every time the model as improved was firstly determined by the number of initial data points, collect using a random policy. The initialization phase has to be chosen not to small to minimize the chance of getting trapped in a local minimum for too long. Afterwards at least on full episode should be taken. We decided to use a short total episode length to reduce the impact of the compound error, the accumulation error following a wrong model, as well known for Dyna-style approaches. The maximal number of steps was ten. During the data collected on the real system, the latest policy was taken with some small noise added to improve the exploration.\\
To decide which MFRL algorithm to use, two main algorithm classes have to be considered: the on and the off-policy algorithms. Online algorithm show a more stable convergence in general, while off-policy algorithms have the advantage that the data buffer can be filled with real data as well. A very attractive off-policy algorithm is the Soft-actor-critic. This algorithm tries to maximize the entropy of the actions to find a good trade-off between exploration and exploitation. On the other hand the on-policy algorithm TRPO provides some theoretical improvement guarantees. In the latter case the policy was improved over the whole training, while for the SAC the policy was reset, to take advantage of the exploration features.

\subsection{Operational deployment}
\NB{Niky} % come abbiamo interagito con FERMI
\subsection{\NB{short FERMI introduction}}
The Free Electron laser Radiation for Multidisciplinary Investigation  better known as the FERMI, is the seeded free-electron laser facility next to the third-generation synchrotron facility ELETTRA at the Elettra Sinchrotrone Trieste laboratories.
A free-electron laser is a fourth-generation light source where the lasing medium consists of very-high-speed electron moving freely through a magnetic structure. The FERMI peculiarity is given by the usage of an external seeding source that provides several advantages, as the increased stability in pulse and photon energy, reduced size of the device, improved longitudinal coherence, more control on the pulse structure, and improved temporal synchronization with external timing systems.

The external optical laser signal provide is contribution to the FEL process in the modulator where it interacts with the relativistic electron beam modulating it in energy. The modulation in energy is the converted into a charge modulation in the dispersive section, and finally the density modulated beams radiation is amplified in the radiators section. The importance of ensuring the best possible overlapping between the seed laser and the electron beam in the modulator is therefore evident.

For this reason the proposed study focuses on the control of the seed laser trajectory in the modulator, looking at FERMI performance as a reference.

\subsection{\NB{our system: the modulator and the seed laser}}
A more detailed description of the alignment process in the modulator is here provided.
The most critical parameters in a seeded free-electron laser are the temporal and transverse overlap of the electron and laser beams in the modulator magnetic section. 
The problem is simplified by keeping constant the trajectory of the electron beam and the mechanical delay line that controls the temporal alignment.
The final problem faced consists in optimising the seed laser trajectory to match the electron beam and consequently increase the intensity of the FEL radiation.



\section{Proof-of-principle application of RL Agent at FERMI trajectory correction}
\NB{Niky}


\subsection{Experiment results from FERMI RL tests}
As discussed in the previous sections, several tests were performed on the FERMI XFEL.
The main purpose was to test the newly implemented algorithms on a real system to evaluate their operational feasibility. The \emph{NAF2} algorithm, as a representative for highly sample efficient MFRL algorithms, was tested first.
\subsection{MFRL tests}
In total four tests were carried out, two using a single network and two using the double network architecture. In both cases \emph{smoothing} was applied, as described in \cref{appendix:naf2}.\\
\Cref{fig:NAF_training} displays the results, averaged over the two test. A training of 100 episodes (shades area) is followed by a verification of 50 steps. in the upper figure the number of iterations per episode is plotted including the cumulative number of steps.
In the verification phase both algorithms show a similar performance, while the double network needs less training steps, and reveals a more stable overall performance.\\
Additionally the convergence metrics of the two algorithms is plotted in \cref{fig:NAF_convergence} against the number of training steps. The blue curves shows the Bellmann error, which is comparable. The state-value function, which is a direct output of the neural net (\cref{eq:state-action-value-approxiation}), converges to a reasonable value for the double network within the shown 700 steps, whereas the single network tends to overestimate the value. In this case convergence is reached ~1400 steps.\\
\subsection{MBRL tests}
The second test campaign was employing the \emph{AE-DYNA} algorithm, as representative for pure a MBRL algorithms. Two variants were implemented: the \emph{ME-TRPO} variant and the \emph{AE-DYNA SAC} variant. In both cases an ensemble of three network was used and the epistemic and aleatoric uncertainty is measured via the anchor-ensemble technique as discussed in \cref{ss:critical_design}. On top early stopping was used.
\\ The first uses the trust region policy optimization -\emph{TRPO}- \cite{Schulman2015} to train the controller. The TRPO monotonically converges to better policy and this property is exploited in the training. The convergence property can be seen in \cref{fig:ME-TRPO_observables}. In the upper figure the total reward per batch is shown, as well as the number of data points used in the training of the dynamics model. In the lower plot the average cumulative reward as achieved by the TRPO on the individual models in the ensemble on a number of 10 episodes is drawn. The shade area shows the corresponding standard deviation to indicate the uncertainty of the dynamics model. To measure the convergence of the TRPO the logarithm of the standard deviation of the distribution drawn trajectories $p_\traj$ is also visualized. Here the training was stopped after 450 steps acquiring 25 steps each dynamics training. In the verification, as can be seen in \cref{fig:ME-TRPO_verification}, all of the 50 episodes where successfully finished after a few steps. \\
Secondly, the \emph{AE-DYNA SAC} was tested, which uses the soft actor critic -SAC- algorithm \cite{Haarnoja2018a}. The SAC not only tries to maximise $J$ \cref{eq:cumulative_reward}, but also simultaneously but weights on the maximisation of the entropy in the action space. In this way exploration is encouraged to avoid getting stuck in a local optimum. In this test the controller is reset each time, when new data for the dynamics model training is acquired. Consequently, the performance drops each time, the dynamics model was retrained, as shown in \cref{fig:AE-DYNA_observables}. Chances to get trapped in a local optimum are smaller by using this strategy of training.
In difference to the first MBRL test, the batches of when acquiring new data are 50 with an initial random walk of 100 steps. The number of initial steps was chosen carefully large enough, because the convergence is slowed down and there is the risk that the training becomes unfeasibly. The training was stopped after the acquisition of 500 data points and a verification was done as in the first test. Again the success is 100\%, but the number of iterations per step is smaller. It might be a result of the bigger number of data points, but in general, this method showed a better asymptotic performance than the $ME-TRPO$ variant.
\begin{figure}[!h]
	\centering
	\includegraphics*[width=0.5\textwidth]{Figures/FERMI_all_experiments_NAF_episodes.pdf}
	\caption{The training on the \emph{NAF2} in the FERMI FEL.}
	\label{fig:NAF_training}
\end{figure}
\begin{figure}[!h]
	\centering
	\includegraphics*[width=0.5\textwidth]{Figures/FERMI_all_experiments_NAF_convergence.pdf}
	\caption{The training .}
	\label{fig:NAF_convergence}
\end{figure}

\begin{figure}[!h]
	\centering
	\input{Figures/SL_Alignment_Scheme}
	\caption{The training .}
	\label{fig:comparsion_per}
\end{figure}




\begin{figure}[!h]
	\centering
	\includegraphics*[width=0.5\textwidth]{Figures/AE-DYNA_observables.pdf}
	\caption{The training .}
	\label{fig:AE-DYNA_observables}
\end{figure}

\begin{figure}[!h]
	\centering
	\includegraphics*[width=0.5\textwidth]{Figures/AE-DYNA_verification.pdf}
	\caption{The training .}
	\label{fig:AE-DYNA_verification}
\end{figure}


\begin{figure}[!h]
	\centering
	\includegraphics*[width=0.5\textwidth]{Figures/ME-TRPO_observables.pdf}
	\caption{The training .}
	\label{fig:ME-TRPO_observables}
\end{figure}
\begin{figure}[!h]
	\centering
	\includegraphics*[width=0.5\textwidth]{Figures/ME-TRPO_verification.pdf}
	\caption{The training .}
	\label{fig:ME-TRPO_verification}
\end{figure}


\section{Discussion and outlook}

\section{Conclusions}


\appendix
\section{A non-linear standard control problem}
To provide some transparency of these studies for other labs we provide results on a famous classical standard control problem \cite{Furutaa}, the \emph{inverted pendulum}. It is a non-linear low dimensional unsolved continuous control problem. Unsolved means there is no threshold for the reward to terminate an episode. The episode length is set to 200 steps. In the following several tests were carried out on the \emph{inverted pendulum} to demonstrate the improvements of the selected algorithms, mainly in terms of noise handling. This is of importance when dealing with measurements on real systems.

\subsection{NAF2 details}\label{appendix:naf2}
%We add artificial Gaussian noise $\gauss(0,0.05)$ on the state.
 We compare the different \emph{NAF} variants: \emph{smoothing-double}, \emph{smoothing single}, \emph{standart}. The smoothing adds a small clipped noise on the actions to stabilize the network training as:
 \begin{equation}
 	a'(s') = \text{clip}\left(\mu_{\theta_{\text{targ}}}(s') + \text{clip}(\epsilon,-c,c), a_{Low}, a_{High}\right),
 \end{equation}
 	where $\epsilon \sim \mathcal{N}(0, \sigma)$. This method was used already in \cite{fujimoto2018addressing} to improve the deterministic policy gradient \cite{Silver2014}.
 The double network was used in \cite{fujimoto2018addressing,Haarnoja2018a} and is done in the following way:
 \begin{align}
y(r,s',d) = r + \gamma (1 - d) \min_{i=1,2} Q_{\phi_{i, \text{targ}}}(s', a'(s'))
 \end{align}
and then both are learned by regressing to this target:
\begin{align}
	L(\phi_1, {\mathcal D}) = \mathbb E_{(s,a,r,s',d) \sim {\mathcal D}}{
		\Bigg( Q_{\phi_1}(s,a) - y(r,s',d) \Bigg)^2
	}
\end{align}
and the policy is obtained via:
\begin{align}
\max_{\theta} \underset{s \sim {\mathcal D}}{{\mathrm E}}\left[ Q_{\phi_1}(s, \mu_{\theta}(s)) \right].
\end{align}
The results are shown in \cref{fig:comparsion_smoothing}. One sees the cumulative reward per episode for a training of a total of 50 episodes. The curve labelled \emph{clipping} corresponds to the double network including the smoothing method and shows the best stability during the training yielding quickly a high reward. Also the single network shows good and comparable performance, except for the stability. The worst performance is achieved without smoothing and a single network, nevertheless the result is competing with state of the art model free methods as \cite{BarthMaron2018} as the benchmark in the \emph{leaderboard} of openai gym \cite{Brockman2016}.
\begin{figure}[!h]
	\centering
	\includegraphics*[width=0.5\textwidth]{Figures/Comparison_smoothing}
	\caption{Cumulative reward of different NAF implementations as discussed in the text.}
	\label{fig:comparsion_smoothing}
\end{figure}
\subsection{The impact of noise}
A test adding artificial Gaussian noise $\epsilon \sim \mathcal{N}(0, \sigma)$ with $\sigma=0.05$ in the normalized observation space on the states is presented in \cref{fig:comparsion_noise}. There the difference of the three methods becomes even more evident.

\begin{figure}[!h]
	\centering
	\includegraphics*[width=0.5\textwidth]{Figures/Comparison_noise}
	\caption{Cumulative reward of different NAF implementations with artificial noise as discussed in the text.}
	\label{fig:comparsion_noise}
\end{figure}

\section{Theoretical aspects on the AE-DYNA on FERMI FEL}
In this section we want to discuss some theoretical aspects concerning the AE-DYNA approach. A simulation was used to obtain the presented analysis. \\
One issue of MBRL methods is the asymptotic performance. Altough good results were obtained using the AE-DYNA, there is a difference between the MFRL and MBRL. We call it the \emph{reality gap}. There are ideas to attack the problem e.g. by learning a meta-policy \cite{Clavera2018} followed by a fine tuning on the residual physics, which minimizes the \emph{reality gap} ot training on a simulator and closing than with a small amount of training iterations on the real system the gap \cite{Zeng2019}.
\begin{figure}[!h]
	\centering
	\includegraphics*[width=0.5\textwidth]{Figures/Learning_evolution}
	\caption{The training .}
	\label{fig:Learning_evolution}
\end{figure}

% The \nocite command causes all entries in a bibliography to be printed out
% whether or not they are actually referenced in the text. This is appropriate
% for the sample file to show the different styles of references, but authors
% most likely will not want to use it.
% \nocite{*}
% \bibliography{apssamp}% Produces the bibliography via BibTeX.
 \bibliography{tex/Bibliography}


% \bibliography{bibliography}
%\addcontentsline{toc}{chapter}{Bibliography}
%
%\begin{thebibliography}{9}
%
%\bibitem{lsa} D. Jacquet, R. Gorbonosov, G. Kruk, "LSA - The High Level Application Software of the LHC and its Performance during the first 3 years of Operation", ICALEPS, San Francisco, CA, USA, 6 - 11 Oct 2013, pp.thppc058.
%
%\bibitem{multiobjective-optimization} A. Edelen, N. Neveu, M. Frey, Y. Huber, C. Mayes, and A. Adelmann, "Machine learning for orders of magnitude speedup in multiobjective optimization of particle accelerator systems", Phys. Rev. Accel. Beams 23, 044601, 2020.
%
%\bibitem{collimator-alignment} G. Azzopardi, A. Muscat, G. Valentino, S. Redaelli, B. Salvachua, "Operational results of LHC collimator alignment using machine learning", Proc. IPAC'19, Melbourne, Australia, pp. 1208-1211, 2019.
%
%\bibitem{SPSAlignment} %InProceedings (Hirlaender2019)
% S. Hirlaender, M. Fraser, B. Goddard , V. Kain, J. Prieto, L. Stoel, M. Szakaly, F. Velotti, "Automatisation of the SPS ElectroStatic Septa Alignment",  
%in 10th Int. Partile Accelerator Conf.(IPAC'19), 2019, p. 4001-4004.
%
%\bibitem{liu} E. Shaposhnikova et al., "LHC Injectors Upgrade (LIU) Project at CERN", 7$^{th}$ International Particle Accelerator Conference, Busan, Korea, 8 - 13 May 2016, pp.MOPOY059, \url{https://doi.org/10.18429/JACoW-IPAC2016-MOPOY059}.
%
%\bibitem{awake} E. Adli, A. Ahuja, O. Apsimon, R. Apsimon, A.-M. Bachmann, D. Barrientos, F. Batsch, J. Bauche, V. B. Olsen, M. Bernardini \textit{et al.}, “Acceleration of electrons in the plasma wakefield of a proton bunch,” Nature 561, 363–367 (2018), \url{https://doi.org/10.1038/s41586-018-0485-4)}.
%
%\bibitem{scheinker} A. Scheinker \textit{et al.}, "Online multi-objective particle accelerator optimization of the AWAKE electron beam line for simultaneous emittance and orbit control", AIP Advances 10, 055320 (2020), \url{https://doi.org/10.1063/5.0003423}.
%
%\bibitem{barto-sutton} R. Sutton, A. Barto, "Introduction to Reinforcement Learning",  MIT Press, Cambridge, MA, USA, 2018.
%
%\bibitem{LQR} H. Kwakernaak, R. Sivan, "Linear Optimal Control Systems", Wiley-Interscience, 1972.
%
%\bibitem{SBP} A. Nagabandi, G. Kahn, R. S. Fearing, S. Levine, "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning", arXiv:1708.02596, 2017. 
%
%\bibitem{DYNA} R. Sutton, "Dyna, an integrated architecture for learning, planning, and reacting", AAAI Spring Symposium, pp. 151-155, 1991, \url{https://doi.org/10.1145/122344.122377}.
%
%\bibitem{benchmarkmbrl} T. Wang et al., “Benchmarking Model-Based Reinforcement Learning”,  \url{https://arxiv.org/abs/1907.02057v1}, 2019-07-03.
%
%\bibitem{dqn} V. Mnih \textit{et al.}, "Human-level control through deep reinforcement learning", Nature 518, 529–533 (2015).
%
%\bibitem{ddpg} T. Lillicrap~\textit{et al.}, "Continuous control with deep reinforcement learning", Proc. ICLR 2016 \url{https://arxiv.org/abs/1509.02971}.
%
%\bibitem{naf} S. Gu, T. Lillicrap, I. Sutskever, S. Levine, "Continuous Deep Q-Learning with Model-based Acceleration", in Proc. 33rd International Conference on Machine Learning, New York, NY, USA, 2016.
%
%\bibitem{td3} S. Fujimoto, H. van Hoof, D. Meger, "Addressing Function Approximation Error in Actor-Critic Methods", arXiv:1802.09477, 2018.
%
%\bibitem{deeppilco} Y. Gal, R. McAllister, and C. E. Rasmussen, “Improving PILCO with Bayesian neural network dynamics models”, in Data-Efficient Machine Learning workshop, ICML, 2016, vol. 4, p. 34.
%
%\bibitem{per} T. Schaul, J. Quan, I. Antonoglou, D. Silver, "Prioritized Experience Replay", arXiv:1511.05952, 2015-11-18.
%
%\bibitem{pernafgit} S. Hirlaender, \textit{PER-NAF} available at \url{https://github.com/MathPhysSim/PER-NAF/} and
%\url{https://pypi.org/project/pernaf/}.
%\bibitem{pyjapc} "pyjapc" available at \url{https://pypi.org/project/pyjapc/}.
%
%\bibitem{openai_gym} \url{http://gym.openai.com}
%
%\bibitem{madx} MAD-X documentation and source code available at \url{https://mad.web.cern.ch/mad/}.
%
%\bibitem{linac4} G. Bellodi, "Linac4 Commissioning Status and Challenges to Nominal Operation", 61$^st$ ICFA Advanced Beam Dynamics Workshop on High-Intensity and High-Brightness Hadron Beams, Daejeon, Korea, 17 - 22 Jun 2018, pp.MOA1PL03, \url{https://doi.org/10.18429/JACoW-HB2018-MOA1PL03}.
%
%\bibitem{auto_matching} F. Velotti, B. Goddard et al., "Automatic AWAKE electron beamline setup using unsupervised machine learning", to be submitted to Phys. Rev. Accel. Beams.
%
%\bibitem{Kumar_1} A. Kumar, A. Gupta, S. Levine, “DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction,” arXiv:2003.07305, 2020-03-16.
%
%\bibitem{Kumar_2} A. Kumar, J. Fu, G. Tucker, S. Levine, “Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction,” arXiv:1906.00949, 2019-06-03.
%
%% NEW REFERENCES
%\bibitem[bruchon2020basic]{Bruchon, Niky, et al. "Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser." Electronics 9.5 (2020): 781.}
%% https://www.mdpi.com/2079-9292/9/5/781/htm
%
%
%
%\end{thebibliography}

%%%%% CLEAR DOUBLE PAGE!
\newpage{\pagestyle{empty}\cleardoublepage}

\end{document}
%
% ****** End of file apssamp.tex ******
