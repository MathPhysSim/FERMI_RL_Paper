@FOOTNOTE{Note1,key="Note1",note="in the normalized state-space"}
@FOOTNOTE{Note2,key="Note2",note="$\protect \vec  {Q}_\theta $ denotes the Q-function $Q_\theta $ represented as a vector of length $|\protect \mathcal  {S}|\times |\protect \mathcal  {A}|$."}
@FOOTNOTE{Note3,key="Note3",note="In some applications $r$ can be deduced from the current state or the state change. In such a situation the modelling can be simplified to model directly $\protect \hat  T$"}
@FOOTNOTE{Note4,key="Note4",note="Considering the training is stochastic with random initial conditions, by chance, the training of the double network variant started above the threshold in both cases."}
@FOOTNOTE{Note5,key="Note5",note="The states are clipped to $[0,1]$."}
@CONTROL{REVTEX42Control}
@CONTROL{apsrev42Control,author="08",editor="1",pages="0",title="0",year="1"}
